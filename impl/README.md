# The `ZENITH` Controller Implementation

This directory contains the implementation of `ZENITH`, as generated by our `NADIR` codebase.

This code comprises of mainly four pieces:
- **Generated Code By `NADIR`**: `NADIR` will consume our PlusCal algorithm as described in the
`specs` directory, and output a Python implementation of each process which can be executed
within our `NADIR` runtime for python.
- **`NADIR` Runtime For Python (`pynadir`)**: `pynadir` spawns each process individually and creates
the initial context that these processes need to run (like how to access the NIB, logging functionalities, etc.)
- **OpenFlow Frontend and Protocol Description**: Our PlusCal algorithm has no notion of what _protocol_ is used to
communicate with the switches, but `pynadir` has well-defined interfaces for plugging-in such descriptions. We have
borrowed the description of OpenFlow from the [RYU](https://github.com/faucetsdn/ryu/tree/master) controller, but
our OpenFlow frontend is written from scratch.
- **Configuration Scripts**: Many of these scripts are there to quickly setup the NIB with OPs and initial topology
state so that we can test the controller.

```
impl
  |
  +---- apps                (GENERATED) Implementation of our application processes
  |                         as described in the PlusCal algorithm (namely TE).
  |
  +---- atomics             (GENERATED) Model value, operator and macro definitions
  |                         used in our PlusCal algorithm. The directory contains
  |                         manually written patches for better performance.
  |
  +---- configurations      Contains configuration settings for spawning NADIR
  |                         processes with our Python runtime for NADIR.
  |
  +---- core                (GENERATED) The ZENITH-core processes (OFC and RC)
  |
  +---- frontend            Our OpenFlow frontend, used to communicate with OVS instances
  |                         to emulate a network for our controller.
  |
  +---- nib                 Definitions for our NIB database and message broker (essentially
  |                         wrappers for MongoDB and RabbitMQ).
  |
  +---- openflow            Definitions for parsing OpenFlow packets (borrowed from the
  |                         RYU controller project)
  |
  +---- pynadir             Our Python runtime for NADIR
  |
  +---- tests               Small tests (for checking pynadir in particular)
  |
  +---- utils               Utility scripts for quickly testing the controller with emulated
                            or fake networks
```

## Setting Up

Assuming a Linux machine with docker:

```
# Install all of `pynadir` and NIB dependencies
python3 -m pip install -r requirements.txt

# Pull docker images of MongoDB and RabbitMQ
docker pull mongo
docker pull rabbitmq:3

# Run the containers (we use the name `nib-queue` for the broker and `nib-db` for the DB)
docker run -d --name nib-db -p 27017:27017 mongo:latest mongod --replSet nibRS
docker run -d --name nib-queue -p 5672:5672 rabbitmq:3

# Initiate a replication set for our NIB database
docker exec -it nib-db mongosh --eval "rs.initiate({_id: "nibRS", members: [{_id: 0, host: "127.0.0.1:27017"}, {_id: 1, host: "127.0.0.1:27018"}, {_id: 2, host: "127.0.0.1:27019"}]})"
```

In case you wish to use OVS, then Mininet must be installed as well:
```
sudo apt-get install mininet
```

> **Note:** We only support OpenFlow 1.3, thus make sure to specify the protocol version when using Mininet

# Example Run

We provide a *fake switch* in the `utils` directory that can be used to communicate with the controller without OpenFlow (it is a manual implementation of our `AbstractSW` specification, with some extra functions). This is the easiest way to quickly check if the controller even functions.

This would require two terminals, in one we can spawn the controller:
```
# Start `ZENITH`, expecting 2 switches with a total of 20 network OPs, each DAG of size
# 10 (so 2 DAGs in total).
python3 -m run --num-switches 2 --num-irs 20 --dag-size 10 --log-level info
```
And in another, one can run our fake switch application:
```
# Spawn 2 fake switches
python3 -m utils.fake_switch --num-switches 2
```

`ZENITH` must automatically detect the switches and begin processing the DAG, and should output a log similar to the following:

![Baseline output of `ZENITH`](misc/converge_1.png)

Take note of the final log output, saying that we have converged. Unless a network or component failure has happened after this message is generated, `ZENITH` must have correctly converged, so we can verify that by inspecting the content of our fake switches.

The `fake_switch.py` script has some commands for checking these:
```
# `show` will output the current state of the TCAM for each switch
>> show
        1: [ 1, 3, 5, 7, 9 ]
        2: [ 2, 4, 6, 8, 10 ]

# `verify` will query the NIB for the expected DAG and compare it to the TCAM contents
>> verify
[ VERIFIER ] The network converged to the correct DAG!

# `pop` can be used to silently remove an OP from the TCAM if needed
>> pop 1 9
Removed IR 9 from TCAM of 1

# The above is completely silent and `ZENITH` will receive no news of it.
# Verifying the current state will fail as-is
>> verify
[ VERIFIER ] The network has not converged to the correct DAG!
[ VERIFIER ] IR 9 should be in the TCAM of 1, but is not!
```

You may also block and unblock switch connections to simulate switch failures. Note that whether or not `ZENITH` knows what to do when a switch is gone, depends if we have configured a DAG for that before the failure happens. In our case we have 2 DAGs, one for a healthy network and one for when switch 1 goes down.
```
>> block 1
[ FAKE-SW  ] Going DOWN for 1 [Loss: 100.0%]
```
By default, `fake_switch` will choose a random subset of the TCAM and delete it for each failed switch (in the above, the whole TCAM will be cleared as the loss percent says).

`ZENITH` is expected to switch the DAG and converge again. Note that this may take slightly longer since it must delete the previous IRs in the swtiches as well.

OFC processes will be titled as well (`zenith-ms`, `zenith-wp` and `zenith-eh`). To test module failures, we can just send a SIGKILL to them and let the runtime respawn them:
```
# This kills the Monitoring Server
pkill -9 zenith-ms
```
`ZENITH` will complain and respawn the process again and continue as usual:
```
[  NADIR   ][ 126.7  ] Spawning Nadir process zenith-ms with PID 1913663
[ WATCHDOG ][126.029 ] Process MonitoringServer was killed by the OS. Policy is to RESPAWN.
[ WATCHDOG ][126.029 ] Respawning dead process MonitoringServer
[ WATCHDOG ][126.032 ] Process MonitoringServer forked with PID 1913663
[  ZENITH  ][ 131.63 ] DAG switch from e20 to 21d
[  ZENITH  ][ 131.99 ] Converged on DAG 21d in 0.36 seconds
```

## With `Open vSwitch`
By default, the OpenFlow frontend is disabled, and `ZENITH` must be told to spawn it with `--with-frontend`:
```
# Spawn ZENITH with OpenFlow frontend, expecting 2 switches
python3 -m run --num-switches 2 --num-irs 20 --dag-size 10 --with-frontend
```
At this point, `ZENITH` will block and wait for all the switches to connect:
```
[   NIB    ] Cleared nib database
[ IR-SETUP ] Inserted 20 IRs and 2 DAGs
[   NIB    ] Cleared Nadir database
[ FRONTEND ] Filled IR cache with 20 cookies!
[ FRONTEND ] Waiting for 2 switches to connect
```

One may elect to connect a Mininet topology to the controller as-is (the frontend by default listens on `127.0.0.2` instead of `127.0.0.1` as we usually have a proxy between the switches and the controller).
```
# Spawn a simple linear topology and connect it to the controller.
sudo mn --topo linear,2 --controller remote,ip=127.0.0.2 --switch ovs,protocols=OpenFlow13
```
Take note of protocol version specified, we have elected to only support on OpenFlow version for simplicity.

To easily induce failures, we have created another script under `utils/privileged_proxy.py`, which when given root access, can spawn and interact with OVS instances and provide the same functionalities of `fake_switch.py`.
```
$ sudo python3 -m utils.privileged_proxy --with cli
Running with root privileges. Datapath operations are available.
tcpproxy>>
```
One can spawn a given number of datapaths and manipulate them:
```
# Create 2 OVS datapaths and connect them to ZENITH
tcpproxy>> makemany 2
[  PROXY   ] Established new connection ('127.0.0.1', 36884) --- ('127.0.0.2', 6653)
[  PROXY   ] Established new connection ('127.0.0.1', 36902) --- ('127.0.0.2', 6653)
```
At this point, `ZENITH` must have caught wind of the switches and attempted to push the DAG:
```
[ FRONTEND ] Topology size reached!
...
[  ZENITH  ][ 4.744  ] New DAG 21d was given to RC
[  ZENITH  ][ 5.523  ] Converged on DAG 21d in 0.78 seconds
```
All usual operations with `fake_switch` can be done with the proxy as well:
```
# Verify convergence
tcpproxy>> verify
[ VERIFIER ] The network converged to the correct DAG!

# Query dataplane state
tcpproxy>> query
1    : 1, 3, 5, 7, 9
2    : 2, 4, 6, 8, 10

# Manipulate the TCAM
tcpproxy>> pop 1 9
tcpproxy>> verify
[ VERIFIER ] The network has not converged to the correct DAG!
[ VERIFIER ] IR 9 should be in the TCAM of 1, but is not!

# Block/Unblock switches
tcpproxy>> block 1
[  PROXY   ] Connection from ('127.0.0.1', 36884) has been blocked ...
tcpproxy>> unblock 1
[  PROXY   ] Established new connection ('127.0.0.1', 41666) --- ('127.0.0.2', 6653)
```
The frontend will report switch events as well:
```
[ FRONTEND ][184.217 ] DP 1 DOWN
[  ZENITH  ][184.584 ] New DAG e20 was given to RC
[  ZENITH  ][184.916 ] Converged on DAG e20 in 0.33 seconds
[ FRONTEND ][203.605 ] DP 1 UP
[  ZENITH  ][204.258 ] DAG switch from e20 to 21d
[  ZENITH  ][204.922 ] Converged on DAG 21d in 0.66 seconds
```
Upon exiting, the proxy will make sure to clear all previous datapaths to prevent resource leaks.